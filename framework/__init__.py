import traceback
import gradio as gr
import json
import uuid
import os
import argparse
import threading
import time
import glob
from dotenv import load_dotenv

# Removed transformers imports
# from transformers import ... 

from framework.local_agent import LocalAgent
from framework.orchestrator_agent import OrchestratorAgent
from framework.analytics_agent import AnalyticsAgent
from framework.tools.gemma_client import get_runtime_history_file
from framework.logger import get_logger
from logging import DEBUG, INFO, WARNING, ERROR
# from framework.utilities.cuda_ram_debug import log_cuda_memory # Optional, likely not needed for Ollama

# Load environment variables from .env file immediately
load_dotenv()

class Framework:
    def __init__(self, *args, **kwargs):
        # Default to a robust model available in Ollama
        self.base_model_name = kwargs.get('base_model_name')
        if not self.base_model_name:
            raise ValueError("Base model name must be provided for Ollama models.")
        
        self.session_id = self._initialize_session()
        
        # Initialize Logger
        self.logger = get_logger("Framework", level=INFO)
        
        # Initialize RAG Engine
        self.rag_engine_enabled = True

        # System messages per agent
        self.persistent_messages = {
            "OrchestratorAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are a Senior Particle Physicist (The Scientist). "
                        "You direct an Analyst Agent to find anomalies in collider data. "
                        "Your Goal: Discover new physics anomalies with >2-4 sigma significance.\n\n"
                        "RECOMMENDED PROTOCOL (Not Strict):\n"
                        "1. PHASE 1 - BOOTSTRAP: Understand the data available to you. \n"
                        "   Scan if the files are available and what they contain using your file listing tool.\n"
                        "   If you need, ask the Analyst about its capabilities and tool variables (what parameters can be set).\n"
                        "2. PHASE 2 - INGESTION: First, ensure the raw data is processed using FastJet. "
                        "   Command the Analyst to run it if not done yet.\n"
                        "3. PHASE 3 - SCOUTING: You cannot scan the entire collider range at once. "
                        "   Command the Analyst to 'Propose Signal Regions'.\n"
                        "   **CRITICAL DISTINCTION**: This step does NOT find anomalies. It only identifies **Valid Search Windows** "
                        "   where the algorithm *can* mathematically function (ensuring sufficient sideband statistics).\n"
                        "   Review the proposed regions carefully.\n You can re-run with different windows if needed.\n"
                        "4. PHASE 4 - HYPOTHESIS: Select a specific Signal Region from the proposals, "
                        "   OR formulate your own if you have external knowledge.\n"
                        "5. PHASE 5 - EXECUTION: Command the Analyst to do necessary studies on your chosen region. \n"
                        "6. PHASE 6 - ANALYSIS: When the Analyst reports a result file "
                        "   IMMEDIATELY read it with your file tool.\n"
                        "7. PHASE 7 - DECISION: Analyze the report data. Use your knowledge tools to research concepts as needed. "
                        "   Decide whether to publish, refine, or re-run with new hypotheses.\n"
                        "8. PHASE 8 - ITERATE OR FINALIZE: Based on your analysis, either iterate with new hypotheses or new signal regions, "
                        "   or finalize your findings with a comprehensive report.\n\n"
                        "## IMPORTANT GUIDELINES:\n"
                        "1. BE CLEAR: Do not forget you guide another LLM agent. Be clear with your instructions. "
                        "   Make sure the paths and requests you provide are correct and unambiguous.\n"
                        "   Meanwhile do not micromanage the Analyst, let it use its autonomy within the rules.\n"
                        "2. BE INFORMED: Before starting costly analysis, use your query_knowledge_base_tool to research physics concepts "
                        "   and your query_gemma_cloud_tool to query the external knowledge base. " 
                        "   Especially if you are stuck, gemma can save you \n"
                        "3. USE THE DATA: Do not make up numbers. Always use data from the reports generated by the Analyst.\n"
                        "4. KNOWLEDGE BASE: To understand the report, use your query_knowledge_base_tool to query the physics knowledge base for relevant information.\n"
                        "5. ANALTYST CAPABILITIES: You can ask the Analyst about its tool capabilities and parameters at any time.\n"
                        "   It have multiple tools and pipelines, so make sure you understand them well to use them effectively.\n\n"
                        "5. DECIDE: \n"
                        "   - If Significance looks convincing and other data seems feasible: Recommend publication.\n"
                        "   - If Significance looks unconvincing: Formulate a new hypothesis (e.g., 'The signal might be softer, let's lower min_pt') and command the Analyst to re-run. You can also ask gemma for opinions about which energy ranges to explore.\n"
                        "   - If Significance is high near band edges: Suggest refining the mass range or scan on new window.\n"
                        "6. WHEN ARE YOU DONE: Only when you have a strong discovery or have exhausted options, start your response with 'FINAL REPORT'. \n"
                        "## What you should report: \n"
                        "1. A p-value associated with the dataset having no new particles (null hypothesis). \n"
                        "2. As complete a description of the new physics as possible. For example: the masses and decay modes of all new particles (and uncertainties on those parameters). \n"
                        "3. How many signal events (+uncertainty) are in the dataset (before any selection criteria). \n\n"
                        "   This is session ID: " + self.session_id + "\n"
                    )
                }
            ],
            "AnalyticsAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are an Expert Research Technician (The Analyst). "
                        "Your user is an another LLM agent called the Orchestrator (The Scientist). "
                        "You are capable of operating the LaCATHODE anomaly detection pipeline, Signal Region Recommendations and Isolation Forest. "
                        "Stick to physics, use units properly, and avoid making up numbers.\n"
                        "Do not ask tool specific questions to the Orchestrator, you have full autonomy to run tools as needed.\n\n"
                        "If instructed, give information about your tools and parameters.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. FILE EXISTENCE: Check if input/output files exist with list_any_folder tool before running tools.\n"
                        "2. NO OVERWRITES: Do not overwrite existing files, unless explicitly instructed by the Orchestrator.\n"
                        "3. NO DUPLICATE RUN IDS: Always use a new run_id for each LaCATHODE tool invocation to avoid conflicts.\n"
                        "4. TOOL USAGE: One tool at a time, wait for completion before next.\n"
                        "5. PIPELINES:" 
                        "5.1. FastJet -> Signal Region Proposal -> Preparation -> Training -> Oracle -> Report Generator.\n"
                        "5.2. You can run Isolation Forest as an independent check after FastJet and if needed Signal Region Proposal.\n"
                        "6. WHEN TO REPORT: If ONLY a specific step is asked, report IMMEDIATELY after that step completes."
                        "If full pipeline is run, report AFTER the Report Generator step completes.\n"
                        "7. PARTIAL RE-RUNS: You can re-run steps to increase report data, but take cost into account.\n"
                        "8. REPORTING: After generating a report, inform the Orchestrator and await further instructions.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. CHECKPOINTS: You are encouraged to STOP and report back after 'FastJet' and 'Region Proposal'. \n"
                        "2. FILE SAFETY: Check file existence before reading. Do not overwrite unless instructed.\n"
                        "3. RUN IDs: If not received by orchestrator, generate unique run_ids for every new training attempt.\n"
                        "4. AUTONOMY: If the Orchestrator says 'Find anomalies' without specifics, you MAY run the full pipeline autonomously.\n\n"
                        "## EXAMPLE-RERUNS:\n"
                        "- If the Orchestrator suggests lowering min_pt, you can re-run Preparation and subsequent steps.\n"
                        "- If the Orchestrator wants a finer mass binning, you can re-run Report Generator with updated parameters.\n"
                        "- If you are not satisfied with the report, you can re-run the report generator\n\n"
                        "Use your own judgement to balance cost vs information gain when re-running tools. \n"
                        f"   This is session ID: {self.session_id}\n"
                    )
                }
            ],
        }

        # Initialize Agents with Model NAME, not object
        self.orchestrator_agent = OrchestratorAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            persistent_messages=self.get_persistent_messages(
                agent="OrchestratorAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        self.analytics_agent = AnalyticsAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            persistent_messages=self.get_persistent_messages(
                agent="AnalyticsAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        # Register peers
        self.orchestrator_agent.register_peer('AnalyticsAgent', self.analytics_agent)
        self.analytics_agent.register_peer('OrchestratorAgent', self.orchestrator_agent)

        # Register agents to framework
        self.agents: dict[str, LocalAgent] = {
            "OrchestratorAgent": self.orchestrator_agent,
            "AnalyticsAgent": self.analytics_agent,
        }

        # Make one active agent
        self.active_agent = self.orchestrator_agent

        # If resume arg provided, trigger resume
        # Resume logic
        resume_job_id = kwargs.get('resume_job_id', None)

        if resume_job_id:
            self.logger.info(f"Resuming from job ID: {resume_job_id}")
            self.trigger_forced_resume(resume_job_id)
            self.forced_resume_in_progress = True
        else:
            self.forced_resume_in_progress = False

        self._last_served_histories = {}

        self.chat_lock = threading.Lock()

        self.is_processing = False
        self.placeholder_text = "Type here..."
    

    def _initialize_session(self):
        counter_path = "session_counter.txt"
        os.makedirs("jobs", exist_ok=True)

        # Read the last number safely
        if os.path.exists(counter_path):
            with open(counter_path, "r") as f:
                try:
                    count = int(f.read().strip())
                except ValueError:
                    count = 0
        else:
            count = 0

        # Increment and save immediately to "lock" this session number
        new_count = count + 1
        with open(counter_path, "w", encoding='utf-8') as f:
            f.write(str(new_count))

        device_tag = os.environ.get("DEVICE_TAG")
        if device_tag:
            session_id = f"{device_tag}_{new_count:03d}"
        else:
            session_id = f"SESS_{new_count:03d}"

        os.environ["FRAMEWORK_SESSION_ID"] = session_id

        return session_id

    def get_persistent_messages(self, agent):
        return self.persistent_messages.get(agent, [])

    def trigger_forced_resume(self, job_id):
        """Loads state from a completed job file to resume."""
        self.logger.info(f"Attempting to resume job {job_id}...")

        result_path = f"jobs/completed/{job_id}.json"

        if not os.path.exists(result_path):
            self.logger.info(f"Result file for job {job_id} not found at {result_path}. Cannot resume.")
            raise FileNotFoundError(f"Result file for job {job_id} not found.")

        with open(result_path, "r") as f:
            try:
                job_data = json.load(f)
            except json.JSONDecodeError as e:
                raise Exception(f"Error decoding JSON from {result_path}: {e}")
            except Exception as e:
                raise Exception(f"Error reading {result_path}: {e}")
        

        if (job_data and 
            job_data.get("original_state") and 
            job_data["original_state"].get("agent_identifier")
            ):
            agent_id = job_data["original_state"]["agent_identifier"]
            agent = self.agents.get(agent_id)
            if not agent:
                self.logger.info(f"Agent {agent_id} not found for resuming job {job_id}.")
                return
            self.logger.info(f"Resuming job {job_id} for agent {agent_id}.")

            agent.pending_job_id = job_id

            def run_generation():
                generator = agent.wait_for_tool_completion(job_id)
                for _ in generator:
                    pass  # Discard output during forced resume
                    # Poller will pick up the updated state

                self.forced_resume_in_progress = False
                self.logger.info(f"Resumed generation for job {job_id} completed.")

            threading.Thread(target=run_generation, daemon=True).start()
        else:
            self.logger.info(f"No valid original state found in job {job_id}. Cannot resume.")
            self.forced_resume_in_progress = False

    def check_background_updates(self, agent_name=None):
        """Polled by Gradio Timer."""
        full_history = self.get_agent_history(agent_name)
        last_served_history = self._last_served_histories.get(agent_name, [])

        # Determine the visual "Tell"
        if self.is_processing:
            placeholder_text = "‚åõ Orchestrator is thinking..."
            interactive_state = False # Block input while busy
        elif self.forced_resume_in_progress:
            placeholder_text = "Resuming..."
            interactive_state = False
        else:
            placeholder_text = "Type here..."
            interactive_state = True

        # If placeholder and history are unchanged and processing is not active, skip update
        if full_history == last_served_history and placeholder_text == self.placeholder_text and not self.is_processing:
                return (gr.skip(), gr.skip())
        
        self._last_served_histories[agent_name] = full_history
        self.placeholder_text = placeholder_text
        
        return (full_history, gr.update(interactive=interactive_state, placeholder=placeholder_text))

    def export_all_histories(self, dir="exports", with_timestamp=True):
        """
        Dumps the full message history of all agents to a JSON file for download.
        """
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        export_directory = os.path.join(dir, self.session_id)
        export_filename = f"session_history.{timestamp}.json" if with_timestamp else "session_history.json"
        filename = os.path.join(export_directory, export_filename)
        os.makedirs(export_directory, exist_ok=True)
        
        # Collect data
        export_data = {
            "framework_version": "1.0",  # Placeholder version
            "base_model": self.base_model_name,
            "rag_engine_enabled": self.rag_engine_enabled,
            "session_id": self.session_id,
            "timestamp": timestamp,
            "agents": {}
        }
        
        for name, agent in self.agents.items():
            export_data["agents"][name] = agent.messages
            
        # Write to file
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(export_data, f, indent=2)
            
        return filename
    
    def import_all_histories(self, filepath):
        """
        Loads message history from a JSON file and restores agent states.
        """
        if not filepath:
            return "No file provided."

        self.logger.info(f"Attempting to import session from {filepath}")

        try:
            with open(filepath, "r", encoding='utf-8') as f:
                data = json.load(f)
            
            # Validate format
            if "agents" not in data:
                return "Error: Invalid session file format (missing 'agents' key)."

            # Restore history for each agent found in the file
            loaded_count = 0
            for agent_name, messages in data["agents"].items():
                if agent_name in self.agents:
                    agent = self.agents[agent_name]
                    agent.flush_messages()  # Clear existing messages
                    for message in messages:
                        agent.append_message(**message)

                    loaded_count += 1
                    self.logger.info(f"Restored {len(messages)} messages for {agent_name}.")
                else:
                    self.logger.warning(f"Agent '{agent_name}' found in file but not in current framework. Skipping.")
            
            return f"Successfully restored session (Timestamp: {data.get('timestamp', 'unknown')}). Loaded {loaded_count} agents."

        except json.JSONDecodeError:
            return "Error: Failed to decode JSON file."
        except Exception as e:
            self.logger.error(f"Import failed: {str(e)}")
            return f"Error importing history: {str(e)}"
    
    def get_gallery_images(self):
        """
        Scans current directory and toolout folders for generated plots.
        """
        # Search for standard plot formats in likely locations
        image_paths = []
        
        # 1. Check Root (where lacathode_roc.png is saved)
        image_paths.extend(glob.glob("*.png"))
        image_paths.extend(glob.glob("*.jpg"))
        
        # 2. Check Tool Output directories recursively
        image_paths.extend(glob.glob("toolout/**/*.png", recursive=True))
        image_paths.extend(glob.glob("toolout/**/*.jpg", recursive=True))
        
        # Sort by modification time (newest first)
        image_paths.sort(key=os.path.getmtime, reverse=True)
        return image_paths
    
    def _background_worker(self, user_input, message_id):
        """Runs the agent loop in a separate thread."""
        self.is_processing = True
        try:
            with self.chat_lock:
                gen = self.active_agent.respond(user_input, message_id)
                for _ in gen: pass
        except Exception as e:
            self.logger.error(f"Background worker crashed: {e}")
            self.logger.error(traceback.format_exc())
        finally:
            # Always log the session state after processing
            logout_dir = os.path.join("logs", "sessions") # Session id appended later
            self.export_all_histories(with_timestamp=False, dir=logout_dir)
            self.is_processing = False
    
    def chat_function(self, user_input):
        """Starts the background task and returns immediately."""
        if not user_input.strip():
             return gr.update(value=""), None
        
        # Save prompt to history
        self._save_prompt_to_history(user_input)

        # Fire and Forget Thread
        message_id = uuid.uuid4().hex
        t = threading.Thread(
            target=self._background_worker, 
            args=(user_input, message_id), 
            daemon=True
        )
        t.start()

        # Return immediately (clears box, does NOT update chatbot directly)
        return gr.update(value="", interactive=True)

    def get_agent_history(self, agent_name):
        agent = self.agents.get(agent_name)
        if agent:
            return agent.messages_to_gradio_history()
        return []

    def _save_prompt_to_history(self, text):
        """Saves unique prompts to a JSON file, moving duplicates to the top."""
        history_file = "exports/prompts_history.json"
        os.makedirs("exports", exist_ok=True)
        
        history = []
        if os.path.exists(history_file):
            with open(history_file, "r") as f:
                history = json.load(f)
        
        # Remove if exists to handle deduplication (will re-add at start)
        history = [p for p in history if p != text]
        history.insert(0, text) # Most recent at the top
        
        # Keep only last 50 prompts to keep the dropdown clean
        history = history[:50]
        
        with open(history_file, "w", encoding='utf-8') as f:
            json.dump(history, f)
        
        return history

    def _get_prompt_history(self):
        """Loads prompt history for the dropdown."""
        history_file = "exports/prompts_history.json"
        if os.path.exists(history_file):
            with open(history_file, "r") as f:
                return json.load(f)
        return []
    
    def run_interactive(self, port=7860):
        with gr.Blocks(fill_height=True) as demo:
            with gr.Tabs():
                    with gr.Tab("Command Center", scale=1):
                        with gr.Row(scale=1):
                            with gr.Column(scale=1):
                                gr.Markdown("### üß† Orchestrator (Scientist)")
                                chatbot_active = gr.Chatbot(
                                    value=self.get_agent_history(self.active_agent.__class__.__name__), 
                                    type="messages", 
                                    label="Orchestrator",
                                    scale=1,
                                    autoscroll=False,
                                    latex_delimiters=[
                                        {"left": "$$", "right": "$$", "display": True},
                                        {"left": "$", "right": "$", "display": False},
                                    ],
                                )
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### üõ†Ô∏è Analytics (Technician)")
                                chatbot_side = gr.Chatbot(
                                    value=self.get_agent_history("AnalyticsAgent"), 
                                    type="messages",
                                    label="Analytics",
                                    scale=1,
                                    autoscroll=False,
                                    #latex_delimiters=[("$$", "$$"), ("$", "$")],
                                )

                        with gr.Row(scale=0):
                            with gr.Column(scale=4):
                                msg = gr.Textbox(label="Your Input", placeholder="Type here...", lines=6)
                            
                            with gr.Column(scale=1):
                                submit_btn = gr.Button("Submit", variant="primary")

                                export_btn = gr.DownloadButton("üíæ Download Full Session History")

                                prompt_history_dropdown = gr.Dropdown(
                                    choices=self._get_prompt_history(),
                                    label="Latest Prompts",
                                    interactive=True
                                )

                            # Hook up the export button
                            export_btn.click(
                                fn=self.export_all_histories,
                                inputs=None,
                                outputs=export_btn
                            )

                            # When choosing from dropdown, update the textbox
                            prompt_history_dropdown.change(
                                fn=lambda x: x,
                                inputs=[prompt_history_dropdown],
                                outputs=[msg]
                            )

                            # When submitting, refresh the dropdown list
                            # Update your existing msg.submit and submit_btn.click outputs

                            def refresh_dropdown():
                                return gr.update(choices=self._get_prompt_history())
                            
                            submit_event = msg.submit(
                                fn=self.chat_function, 
                                inputs=[msg], 
                                outputs=[msg] 
                            )

                            submit_event.then(
                                fn=refresh_dropdown,
                                inputs=None,
                                outputs=[prompt_history_dropdown]
                            )

                            click_event = submit_btn.click(
                                fn=self.chat_function, 
                                inputs=[msg], 
                                outputs=[msg] 
                            )

                            click_event.then(
                                fn=refresh_dropdown,
                                inputs=None,
                                outputs=[prompt_history_dropdown]
                            )

                            # Timer for background updates (Resume logic)
                            # We can be quite aggressively polling since if there are no changes,
                            # check_background_updates will skip updating the component (no flicker)
                            active_agent_polling_timer = gr.Timer(value=0.3, active=True)
                            active_agent_polling_timer.tick(
                                fn=lambda: self.check_background_updates(self.active_agent.__class__.__name__), 
                                inputs=None,
                                outputs=[chatbot_active, msg]
                            )

                            side_agent_polling_timer = gr.Timer(value=5.0, active=True)
                            side_agent_polling_timer.tick(
                                fn=lambda: self.check_background_updates("AnalyticsAgent"),
                                inputs=None,
                                outputs=[chatbot_side, msg]
                            )

                    with gr.Tab("Visualizations", scale=1):
                        gr.Markdown("### Generated Plots & Figures")
                        refresh_btn = gr.Button("üîÑ Refresh Gallery")
                        gallery = gr.Gallery(
                            label="Generated Images", 
                            show_label=False, 
                            elem_id="gallery", 
                            columns=[3], 
                            rows=[2], 
                            object_fit="contain", 
                            height="auto"
                        )
                        
                        # Load images on click
                        refresh_btn.click(
                            fn=self.get_gallery_images,
                            inputs=None,
                            outputs=gallery
                        )
                    
                    with gr.Tab("Tools", scale=1):
                        # Import Session
                        gr.Markdown("### üìÇ Import Previous Session")
                        def handle_import(file_obj):
                            # Run the import
                            status = self.import_all_histories(file_obj)
                            
                            # Return updates for both chatbots to show restored history immediately
                            return (
                                gr.update(placeholder=f"System: {status}"), # Feedback in the text box
                                self.get_agent_history(self.active_agent.__class__.__name__), # Refresh Active
                                self.get_agent_history("AnalyticsAgent") # Refresh Side
                            )

                        import_btn = gr.File(
                            label="üìÇ Restore Session", 
                            file_types=[".json"], 
                            file_count="single",
                            type="filepath" # Passes the file path string to the function
                        )
                                    
                        import_btn.upload(
                            fn=handle_import,
                            inputs=[import_btn],
                            outputs=[msg, chatbot_active, chatbot_side]
                        )
                    
                        gr.Markdown("### üìö External Knowledge Logs (Gemma)")
                        gr.Markdown("This log shows the specific questions asked to the external knowledge base and the responses received.")
                        
                        refresh_tools_btn = gr.Button("üîÑ Refresh Logs")
                        tools_log_display = gr.Markdown("No logs yet...")
                        
                        # Load initial state
                        demo.load(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )

                        # Manual Refresh
                        refresh_tools_btn.click(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )
            demo.load(
                fn=lambda: self.check_background_updates(self.active_agent.__class__.__name__),
                inputs=None,
                outputs=[chatbot_active, msg]
            )

            demo.load(
                fn=lambda: self.check_background_updates("AnalyticsAgent"),
                inputs=None,
                outputs=[chatbot_side, msg]
            )

        demo.launch(share=False, server_port=port)