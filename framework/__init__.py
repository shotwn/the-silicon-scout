import gradio as gr
import json
import uuid
import os
import argparse
import threading
import time
import glob

# Removed transformers imports
# from transformers import ... 

from framework.orchestrator_agent import OrchestratorAgent
from framework.analytics_agent import AnalyticsAgent
from framework.tools.gemma_client import get_runtime_history_file
from framework.logger import get_logger
from logging import DEBUG, INFO, WARNING, ERROR
# from framework.utilities.cuda_ram_debug import log_cuda_memory # Optional, likely not needed for Ollama

class Framework:
    def __init__(self, *args, **kwargs):
        # Default to a robust model available in Ollama
        self.base_model_name = kwargs.get('base_model_name')
        if not self.base_model_name:
            raise ValueError("Base model name must be provided for Ollama models.")
        
        # Initialize Logger
        self.logger = get_logger("Framework", level=INFO)
        
        # Initialize RAG Engine
        self.rag_engine_enabled = True

        # Initial messages per agent
        self.default_initial_messages = {
            "OrchestratorAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are a Senior Particle Physicist (The Scientist). "
                        "You direct an Analyst Agent to find anomalies in collider data. "
                        "Your Goal: Discover new physics anomalies with >2-4 sigma significance.\n\n"
                        "RECOMMENDED PROTOCOL (Not Strict):\n"
                        "1. PHASE 1 - BOOTSTRAP: Understand the data available to you. \n"
                        "   Scan if the files are available and what they contain using your file listing tool.\n"
                        "   If you need, ask the Analyst about its capabilities and tool variables (what parameters can be set).\n"
                        "2. PHASE 2 - INGESTION: First, ensure the raw data is processed using FastJet. "
                        "   Command the Analyst to run it if not done yet.\n"
                        "3. PHASE 3 - SCOUTING: You cannot scan the entire collider range at once. "
                        "   Command the Analyst to 'Propose Signal Regions'.\n"
                        "   **CRITICAL DISTINCTION**: This step does NOT find anomalies. It only identifies **Valid Search Windows** "
                        "   where the algorithm *can* mathematically function (ensuring sufficient sideband statistics).\n"
                        "   Review the proposed regions carefully.\n You can re-run with different windows if needed.\n"
                        "4. PHASE 4 - HYPOTHESIS: Select a specific Signal Region from the proposals, "
                        "   OR formulate your own if you have external knowledge.\n"
                        "5. PHASE 5 - EXECUTION: Command the Analyst to do necessary studies on your chosen region. \n"
                        "6. PHASE 6 - ANALYSIS: When the Analyst reports a result file "
                        "   IMMEDIATELY read it with your file tool.\n"
                        "7. PHASE 7 - DECISION: Analyze the report data. Use your knowledge tools to research concepts as needed. "
                        "   Decide whether to publish, refine, or re-run with new hypotheses.\n"
                        "8. PHASE 8 - ITERATE OR FINALIZE: Based on your analysis, either iterate with new hypotheses or new signal regions, "
                        "   or finalize your findings with a comprehensive report.\n\n"
                        "## IMPORTANT GUIDELINES:\n"
                        "1. BE CLEAR: Do not forget you guide another LLM agent. Be clear with your instructions. "
                        "   Make sure the paths and requests you provide are correct and unambiguous.\n"
                        "   Meanwhile do not micromanage the Analyst, let it use its autonomy within the rules.\n"
                        "2. BE INFORMED: Before starting costly analysis, use your query_knowledge_base_tool to research physics concepts "
                        "and your query_gemma_cloud_tool to query the external knowledge base.\n"
                        "3. USE THE DATA: Do not make up numbers. Always use data from the reports generated by the Analyst.\n"
                        "4. KNOWLEDGE BASE: To understand the report, use your query_knowledge_base_tool to query the physics knowledge base for relevant information.\n"
                        "5. ANALTYST CAPABILITIES: You can ask the Analyst about its tool capabilities and parameters at any time.\n"
                        "   It have multiple tools and pipelines, so make sure you understand them well to use them effectively.\n\n"
                        "5. DECIDE: \n"
                        "   - If Significance looks convincing and other data seems feasible: Recommend publication.\n"
                        "   - If Significance looks unconvincing: Formulate a new hypothesis (e.g., 'The signal might be softer, let's lower min_pt') and command the Analyst to re-run.\n"
                        "   - If Significance is high near band edges: Suggest refining the mass range or scan on new window.\n"
                        "6. WHEN ARE YOU DONE: Only when you have a strong discovery or have exhausted options, start your response with 'FINAL REPORT'. \n"
                        "   Final report should summarize findings, significance, and next steps.\n"
                        "   It should give an results overview of how many signal events might be present in the data and type of these events.\n"
                        "   As well as reasons for confidence in the results.\n"
                        "   Results will be used to measure this framework's performance in a paper.\n\n"
                    )
                }
            ],
            "AnalyticsAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are an Expert Research Technician (The Analyst). "
                        "Your user is an another LLM agent called the Orchestrator (The Scientist). "
                        "You are capable of operating the LaCATHODE anomaly detection pipeline. "
                        "If instructed, give information about your tools and parameters.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. FILE EXISTENCE: Check if input/output files exist with list_any_folder tool before running tools.\n"
                        "2. NO OVERWRITES: Do not overwrite existing files, unless explicitly instructed by the Orchestrator.\n"
                        "3. NO DUPLICATE RUN IDS: Always use a new run_id for each LaCATHODE tool invocation to avoid conflicts.\n"
                        "4. TOOL USAGE: One tool at a time, wait for completion before next.\n"
                        "5. PIPELINES:" 
                        "5.1. FastJet -> Signal Region Proposal -> Preparation -> Training -> Oracle -> Report Generator.\n"
                        "5.2. You can run Isolation Forest as an independent check after FastJet and if needed Signal Region Proposal.\n"
                        "6. WHEN TO REPORT: If ONLY a specific step is asked, report IMMEDIATELY after that step completes."
                        "If full pipeline is run, report AFTER the Report Generator step completes.\n"
                        "7. PARTIAL RE-RUNS: You can re-run steps to increase report data, but take cost into account.\n"
                        "8. REPORTING: After generating a report, inform the Orchestrator and await further instructions.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. CHECKPOINTS: You are encouraged to STOP and report back after 'FastJet' and 'Region Proposal'. \n"
                        "2. FILE SAFETY: Check file existence before reading. Do not overwrite unless instructed.\n"
                        "3. RUN IDs: If not received by orchestrator, generate unique run_ids for every new training attempt.\n"
                        "4. AUTONOMY: If the Orchestrator says 'Find anomalies' without specifics, you MAY run the full pipeline autonomously.\n\n"
                        "5. EASY WITH PYTHON: You have access to a Python REPL tool. Use it for calculations, file inspections, or data manipulations. \n"
                        "   But make sure to not create workloads that will take hours to run, rely on existing tools first. Keep it efficient and quick.\n\n"
                        "## EXAMPLE-RERUNS:\n"
                        "- If the Orchestrator suggests lowering min_pt, you can re-run Preparation and subsequent steps.\n"
                        "- If the Orchestrator wants a finer mass binning, you can re-run Report Generator with updated parameters.\n"
                        "- If you are not satisfied with the report, you can re-run the report generator\n\n"
                        "Use your own judgement to balance cost vs information gain when re-running tools."
                    )
                }
            ],
        }

        # Initialize Agents with Model NAME, not object
        self.orchestrator_agent = OrchestratorAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            initial_messages=self.get_initial_messages(
                agent="OrchestratorAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        self.analytics_agent = AnalyticsAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            initial_messages=self.get_initial_messages(
                agent="AnalyticsAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        # Register peers
        self.orchestrator_agent.register_peer('AnalyticsAgent', self.analytics_agent)
        self.analytics_agent.register_peer('OrchestratorAgent', self.orchestrator_agent)

        # Register agents to framework
        self.agents = {
            "OrchestratorAgent": self.orchestrator_agent,
            "AnalyticsAgent": self.analytics_agent,
        }

        # Make one active agent
        self.active_agent = self.orchestrator_agent

        # If resume arg provided, trigger resume
        # Resume logic
        resume_job_id = kwargs.get('resume_job_id', None)

        if resume_job_id:
            self.logger.info(f"Resuming from job ID: {resume_job_id}")
            self.trigger_forced_resume(resume_job_id)
            self.forced_resume_in_progress = True
        else:
            self.forced_resume_in_progress = False

        self._last_served_histories = {}

        self.chat_lock = threading.Lock()

        self.is_processing = False
        self.placeholder_text = "Type here..."
    

    def get_initial_messages(self, agent):
        return self.default_initial_messages.get(agent, [])

    def trigger_forced_resume(self, job_id):
        """Loads state from a completed job file to resume."""
        self.logger.info(f"Attempting to resume job {job_id}...")

        result_path = f"jobs/completed/{job_id}.json"

        if not os.path.exists(result_path):
            self.logger.info(f"Result file for job {job_id} not found at {result_path}. Cannot resume.")
            raise FileNotFoundError(f"Result file for job {job_id} not found.")

        with open(result_path, "r") as f:
            try:
                job_data = json.load(f)
            except json.JSONDecodeError as e:
                raise Exception(f"Error decoding JSON from {result_path}: {e}")
            except Exception as e:
                raise Exception(f"Error reading {result_path}: {e}")
        

        if (job_data and 
            job_data.get("original_state") and 
            job_data["original_state"].get("agent_identifier")
            ):
            agent_id = job_data["original_state"]["agent_identifier"]
            agent = self.agents.get(agent_id)
            if not agent:
                self.logger.info(f"Agent {agent_id} not found for resuming job {job_id}.")
                return
            self.logger.info(f"Resuming job {job_id} for agent {agent_id}.")

            agent.pending_job_id = job_id

            def run_generation():
                generator = agent.wait_for_tool_completion(job_id)
                for _ in generator:
                    pass  # Discard output during forced resume
                    # Poller will pick up the updated state

                self.forced_resume_in_progress = False
                self.logger.info(f"Resumed generation for job {job_id} completed.")

            threading.Thread(target=run_generation, daemon=True).start()
        else:
            self.logger.info(f"No valid original state found in job {job_id}. Cannot resume.")
            self.forced_resume_in_progress = False

    def check_background_updates(self, agent_name=None):
        """Polled by Gradio Timer."""
        full_history = self.get_agent_history(agent_name)
        last_served_history = self._last_served_histories.get(agent_name, [])

        # Determine the visual "Tell"
        if self.is_processing:
            placeholder_text = "‚åõ Orchestrator is thinking..."
            interactive_state = False # Block input while busy
        elif self.forced_resume_in_progress:
            placeholder_text = "Resuming..."
            interactive_state = False
        else:
            placeholder_text = "Type here..."
            interactive_state = True

        # If placeholder and history are unchanged and processing is not active, skip update
        if full_history == last_served_history and placeholder_text == self.placeholder_text and not self.is_processing:
                return (gr.skip(), gr.skip())
        
        self._last_served_histories[agent_name] = full_history
        self.placeholder_text = placeholder_text
        
        return (full_history, gr.update(interactive=interactive_state, placeholder=placeholder_text))

    def export_all_histories(self):
        """
        Dumps the full message history of all agents to a JSON file for download.
        """
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = os.path.join("exports", f"session_history_{timestamp}.json")
        os.makedirs("exports", exist_ok=True)
        
        # Collect data
        export_data = {
            "timestamp": timestamp,
            "agents": {}
        }
        
        for name, agent in self.agents.items():
            export_data["agents"][name] = agent.messages
            
        # Write to file
        with open(filename, "w") as f:
            json.dump(export_data, f, indent=2)
            
        return filename
    
    def import_all_histories(self, filepath):
        """
        Loads message history from a JSON file and restores agent states.
        """
        if not filepath:
            return "No file provided."

        self.logger.info(f"Attempting to import session from {filepath}")

        try:
            with open(filepath, "r", encoding='utf-8') as f:
                data = json.load(f)
            
            # Validate format
            if "agents" not in data:
                return "Error: Invalid session file format (missing 'agents' key)."

            # Restore history for each agent found in the file
            loaded_count = 0
            for agent_name, messages in data["agents"].items():
                if agent_name in self.agents:
                    self.agents[agent_name].messages = messages
                    loaded_count += 1
                    self.logger.info(f"Restored {len(messages)} messages for {agent_name}.")
                else:
                    self.logger.warning(f"Agent '{agent_name}' found in file but not in current framework. Skipping.")
            
            return f"Successfully restored session (Timestamp: {data.get('timestamp', 'unknown')}). Loaded {loaded_count} agents."

        except json.JSONDecodeError:
            return "Error: Failed to decode JSON file."
        except Exception as e:
            self.logger.error(f"Import failed: {str(e)}")
            return f"Error importing history: {str(e)}"
    
    def get_gallery_images(self):
        """
        Scans current directory and toolout folders for generated plots.
        """
        # Search for standard plot formats in likely locations
        image_paths = []
        
        # 1. Check Root (where lacathode_roc.png is saved)
        image_paths.extend(glob.glob("*.png"))
        image_paths.extend(glob.glob("*.jpg"))
        
        # 2. Check Tool Output directories recursively
        image_paths.extend(glob.glob("toolout/**/*.png", recursive=True))
        image_paths.extend(glob.glob("toolout/**/*.jpg", recursive=True))
        
        # Sort by modification time (newest first)
        image_paths.sort(key=os.path.getmtime, reverse=True)
        return image_paths
    
    def _background_worker(self, user_input, message_id):
        """Runs the agent loop in a separate thread."""
        self.is_processing = True # [NEW] Set flag
        try:
            with self.chat_lock:
                gen = self.active_agent.respond(user_input, message_id)
                for _ in gen: pass
        except Exception as e:
            self.logger.error(f"Background worker crashed: {e}")
        finally:
            self.is_processing = False # [NEW] Reset flag
    
    def chat_function(self, user_input):
        """Starts the background task and returns immediately."""
        if not user_input.strip():
             return gr.update(value=""), None
        
        # Save prompt to history
        self._save_prompt_to_history(user_input)

        # Fire and Forget Thread
        message_id = uuid.uuid4().hex
        t = threading.Thread(
            target=self._background_worker, 
            args=(user_input, message_id), 
            daemon=True
        )
        t.start()

        # Return immediately (clears box, does NOT update chatbot directly)
        return gr.update(value="", interactive=True)

    def get_agent_history(self, agent_name):
        agent = self.agents.get(agent_name)
        if agent:
            return agent.messages_to_gradio_history()
        return []

    def _save_prompt_to_history(self, text):
        """Saves unique prompts to a JSON file, moving duplicates to the top."""
        history_file = "exports/prompts_history.json"
        os.makedirs("exports", exist_ok=True)
        
        history = []
        if os.path.exists(history_file):
            with open(history_file, "r") as f:
                history = json.load(f)
        
        # Remove if exists to handle deduplication (will re-add at start)
        history = [p for p in history if p != text]
        history.insert(0, text) # Most recent at the top
        
        # Keep only last 50 prompts to keep the dropdown clean
        history = history[:50]
        
        with open(history_file, "w") as f:
            json.dump(history, f)
        
        return history

    def _get_prompt_history(self):
        """Loads prompt history for the dropdown."""
        history_file = "exports/prompts_history.json"
        if os.path.exists(history_file):
            with open(history_file, "r") as f:
                return json.load(f)
        return []
    
    def run_interactive(self, port=7860):
        with gr.Blocks(fill_height=True) as demo:
            with gr.Tabs():
                    with gr.Tab("Command Center", scale=1):
                        with gr.Row(scale=1):
                            with gr.Column(scale=1):
                                gr.Markdown("### üß† Orchestrator (Scientist)")
                                chatbot_active = gr.Chatbot(
                                    value=self.get_agent_history(self.active_agent.__class__.__name__), 
                                    type="messages", 
                                    label="Orchestrator",
                                    scale=1
                                )
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### üõ†Ô∏è Analytics (Technician)")
                                chatbot_side = gr.Chatbot(
                                    value=self.get_agent_history("AnalyticsAgent"), 
                                    type="messages",
                                    label="Analytics",
                                    scale=1
                                )

                        with gr.Row(scale=0):
                            with gr.Column(scale=4):
                                msg = gr.Textbox(label="Your Input", placeholder="Type here...", lines=6)
                            
                            with gr.Column(scale=1):
                                submit_btn = gr.Button("Submit", variant="primary")

                                export_btn = gr.DownloadButton("üíæ Download Full Session History")

                                prompt_history_dropdown = gr.Dropdown(
                                    choices=self._get_prompt_history(),
                                    label="Latest Prompts",
                                    interactive=True
                                )

                            # Hook up the export button
                            export_btn.click(
                                fn=self.export_all_histories,
                                inputs=None,
                                outputs=export_btn
                            )

                            # When choosing from dropdown, update the textbox
                            prompt_history_dropdown.change(
                                fn=lambda x: x,
                                inputs=[prompt_history_dropdown],
                                outputs=[msg]
                            )

                            # When submitting, refresh the dropdown list
                            # Update your existing msg.submit and submit_btn.click outputs

                            def refresh_dropdown():
                                return gr.update(choices=self._get_prompt_history())
                            
                            submit_event = msg.submit(
                                fn=self.chat_function, 
                                inputs=[msg], 
                                outputs=[msg] 
                            )

                            submit_event.then(
                                fn=refresh_dropdown,
                                inputs=None,
                                outputs=[prompt_history_dropdown]
                            )

                            click_event = submit_btn.click(
                                fn=self.chat_function, 
                                inputs=[msg], 
                                outputs=[msg] 
                            )

                            click_event.then(
                                fn=refresh_dropdown,
                                inputs=None,
                                outputs=[prompt_history_dropdown]
                            )

                            # Timer for background updates (Resume logic)
                            # We can be quite aggressively polling since if there are no changes,
                            # check_background_updates will skip updating the component (no flicker)
                            active_agent_polling_timer = gr.Timer(value=0.3, active=True)
                            active_agent_polling_timer.tick(
                                fn=lambda: self.check_background_updates(self.active_agent.__class__.__name__), 
                                inputs=None,
                                outputs=[chatbot_active, msg]
                            )

                            side_agent_polling_timer = gr.Timer(value=5.0, active=True)
                            side_agent_polling_timer.tick(
                                fn=lambda: self.check_background_updates("AnalyticsAgent"),
                                inputs=None,
                                outputs=[chatbot_side, msg]
                            )

                    with gr.Tab("Visualizations", scale=1):
                        gr.Markdown("### Generated Plots & Figures")
                        refresh_btn = gr.Button("üîÑ Refresh Gallery")
                        gallery = gr.Gallery(
                            label="Generated Images", 
                            show_label=False, 
                            elem_id="gallery", 
                            columns=[3], 
                            rows=[2], 
                            object_fit="contain", 
                            height="auto"
                        )
                        
                        # Load images on click
                        refresh_btn.click(
                            fn=self.get_gallery_images,
                            inputs=None,
                            outputs=gallery
                        )
                    
                    with gr.Tab("Tools", scale=1):
                        # Import Session
                        gr.Markdown("### üìÇ Import Previous Session")
                        def handle_import(file_obj):
                            # Run the import
                            status = self.import_all_histories(file_obj)
                            
                            # Return updates for both chatbots to show restored history immediately
                            return (
                                gr.update(placeholder=f"System: {status}"), # Feedback in the text box
                                self.get_agent_history(self.active_agent.__class__.__name__), # Refresh Active
                                self.get_agent_history("AnalyticsAgent") # Refresh Side
                            )

                        import_btn = gr.File(
                            label="üìÇ Restore Session", 
                            file_types=[".json"], 
                            file_count="single",
                            type="filepath" # Passes the file path string to the function
                        )
                                    
                        import_btn.upload(
                            fn=handle_import,
                            inputs=[import_btn],
                            outputs=[msg, chatbot_active, chatbot_side]
                        )
                    
                        gr.Markdown("### üìö External Knowledge Logs (Gemma)")
                        gr.Markdown("This log shows the specific questions asked to the external knowledge base and the responses received.")
                        
                        refresh_tools_btn = gr.Button("üîÑ Refresh Logs")
                        tools_log_display = gr.Markdown("No logs yet...")
                        
                        # Load initial state
                        demo.load(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )

                        # Manual Refresh
                        refresh_tools_btn.click(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )

        demo.launch(share=False, server_port=port)