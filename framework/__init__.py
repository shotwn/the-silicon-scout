import gradio as gr
import json
import uuid
import os
import argparse
import threading
import time
import glob

# Removed transformers imports
# from transformers import ... 

from framework.orchestrator_agent import OrchestratorAgent
from framework.analytics_agent import AnalyticsAgent
from framework.tools.gemma_client import get_runtime_history_file
from framework.logger import get_logger
from logging import DEBUG, INFO, WARNING, ERROR
# from framework.utilities.cuda_ram_debug import log_cuda_memory # Optional, likely not needed for Ollama

class Framework:
    def __init__(self, *args, **kwargs):
        # Default to a robust model available in Ollama
        self.base_model_name = kwargs.get('base_model_name')
        if not self.base_model_name:
            raise ValueError("Base model name must be provided for Ollama models.")
        
        # Initialize Logger
        self.logger = get_logger("Framework", level=INFO)
        
        # Initialize RAG Engine
        self.rag_engine_enabled = True

        # Initial messages per agent
        self.default_initial_messages = {
            "OrchestratorAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are a Senior Particle Physicist (The Scientist). "
                        "You direct an Analyst Agent to find anomalies in collider data. "
                        "Your Goal: Discover new physics anomalies with >2-4 sigma significance.\n\n"
                        "RECOMMENDED PROTOCOL (Not Strict):\n"
                        "1. PHASE 1 - BOOTSTRAP: Understand the data available to you. \n"
                        "   Scan if the files are available and what they contain using your file listing tool.\n"
                        "   If you need, ask the Analyst about its capabilities and tool variables (what parameters can be set).\n"
                        "2. PHASE 2 - INGESTION: First, ensure the raw data is processed using FastJet. "
                        "   Command the Analyst to run it if not done yet.\n"
                        "3. PHASE 3 - SCOUTING: You cannot scan the entire collider range at once. "
                        "   Command the Analyst to 'Propose Signal Regions'.\n"
                        "   **CRITICAL DISTINCTION**: This step does NOT find anomalies. It only identifies **Valid Search Windows** "
                        "   where the algorithm *can* mathematically function (ensuring sufficient sideband statistics).\n"
                        "4. PHASE 4 - HYPOTHESIS: Select a specific Signal Region from the proposals, "
                        "   OR formulate your own if you have external knowledge.\n"
                        "5. PHASE 5 - EXECUTION: Command the Analyst to do necessary studies on your chosen region. \n"
                        "6. PHASE 6 - ANALYSIS: When the Analyst reports a result file "
                        "   IMMEDIATELY read it with your file tool.\n"
                        "7. PHASE 7 - DECISION: Analyze the report data. Use your knowledge tools to research concepts as needed. "
                        "   Decide whether to publish, refine, or re-run with new hypotheses.\n\n"
                        "IMPORTANT:\n"
                        "1. BE CLEAR: Do not forget you guide another LLM agent. Be clear with your instructions. "
                        "Make sure the paths and parameters you provide are correct and unambiguous.\n"
                        "2. BE INFORMED: Before starting costly analysis, use your query_knowledge_base_tool to research physics concepts "
                        "and your query_gemma_cloud_tool to query the external knowledge base.\n"
                        "3. USE THE DATA: Do not make up numbers. Always use data from the reports generated by the Analyst.\n"
                        "4. KNOWLEDGE BASE: To understand the report, use your query_knowledge_base_tool to query the physics knowledge base for relevant information.\n"
                        "5. DECIDE: \n"
                        "   - If Significance looks convincing and other data seems feasible: Recommend publication.\n"
                        "   - If Significance looks unconvincing: Formulate a new hypothesis (e.g., 'The signal might be softer, let's lower min_pt') and command the Analyst to re-run.\n"
                        "   - If Significance is high near band edges: Suggest refining the mass range.\n"
                        "6. WHEN ARE YOU DONE: Only when you have a strong discovery or have exhausted options, start your response with 'FINAL REPORT'."
                    )
                }
            ],
            "AnalyticsAgent": [
                {
                    "role": "system", 
                    "content": (
                        "You are an Expert Research Technician (The Analyst). "
                        "Your user is an another LLM agent called the Orchestrator (The Scientist). "
                        "You are capable of operating the LaCATHODE anomaly detection pipeline. "
                        "If instructed, give information about your tools and parameters.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. FILE EXISTENCE: Check if input/output files exist with list_any_folder tool before running tools.\n"
                        "2. NO OVERWRITES: Do not overwrite existing files, unless explicitly instructed by the Orchestrator.\n"
                        "3. NO DUPLICATE RUN IDS: Always use a new run_id for each LaCATHODE tool invocation to avoid conflicts.\n"
                        "4. TOOL USAGE: One tool at a time, wait for completion before next.\n"
                        "5. PIPELINES:" 
                        "5.1. FastJet -> Signal Region Proposal -> Preparation -> Training -> Oracle -> Report Generator.\n"
                        "6. WHEN TO REPORT: If ONLY a specific step is asked, report IMMEDIATELY after that step completes."
                        "If full pipeline is run, report AFTER the Report Generator step completes.\n"
                        "7. PARTIAL RE-RUNS: You can re-run steps to increase report data, but take cost into account.\n"
                        "8. REPORTING: After generating a report, inform the Orchestrator and await further instructions.\n\n"
                        "## EXECUTION RULES:\n"
                        "1. CHECKPOINTS: You are encouraged to STOP and report back after 'FastJet' and 'Region Proposal'. \n"
                        "2. FILE SAFETY: Check file existence before reading. Do not overwrite unless instructed.\n"
                        "3. RUN IDs: If not received by orchestrator, generate unique run_ids for every new training attempt.\n"
                        "4. AUTONOMY: If the Orchestrator says 'Find anomalies' without specifics, you MAY run the full pipeline autonomously.\n\n"
                        "## EXAMPLE-RERUNS:\n"
                        "- If the Orchestrator suggests lowering min_pt, you can re-run Preparation and subsequent steps.\n"
                        "- If the Orchestrator wants a finer mass binning, you can re-run Report Generator with updated parameters.\n"
                        "- If you are not satisfied with the report, you can re-run the report generator\n\n"
                        "Use your own judgement to balance cost vs information gain when re-running tools."
                    )
                }
            ],
        }

        # Initialize Agents with Model NAME, not object
        self.orchestrator_agent = OrchestratorAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            initial_messages=self.get_initial_messages(
                agent="OrchestratorAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        self.analytics_agent = AnalyticsAgent(
            model_name=self.base_model_name, # Changed arg name
            # tokenizer=self.tokenizer,      # Removed
            initial_messages=self.get_initial_messages(
                agent="AnalyticsAgent"
            ),
            rag_engine_enabled=self.rag_engine_enabled,
        )

        # Register peers
        self.orchestrator_agent.register_peer('AnalyticsAgent', self.analytics_agent)
        self.analytics_agent.register_peer('OrchestratorAgent', self.orchestrator_agent)

        # Register agents to framework
        self.agents = {
            "OrchestratorAgent": self.orchestrator_agent,
            "AnalyticsAgent": self.analytics_agent,
        }

        # Make one active agent
        self.active_agent = self.orchestrator_agent

        # If resume arg provided, trigger resume
        # Resume logic
        resume_job_id = kwargs.get('resume_job_id', None)

        if resume_job_id:
            self.logger.info(f"Resuming from job ID: {resume_job_id}")
            self.trigger_forced_resume(resume_job_id)
            self.forced_resume_in_progress = True
        else:
            self.forced_resume_in_progress = False
    

    def get_initial_messages(self, agent):
        return self.default_initial_messages.get(agent, [])


    def trigger_forced_resume(self, job_id):
        """Loads state from a completed job file to resume."""
        self.logger.info(f"Attempting to resume job {job_id}...")

        result_path = f"jobs/completed/{job_id}.json"

        if not os.path.exists(result_path):
            self.logger.info(f"Result file for job {job_id} not found at {result_path}. Cannot resume.")
            raise FileNotFoundError(f"Result file for job {job_id} not found.")

        with open(result_path, "r") as f:
            try:
                job_data = json.load(f)
            except json.JSONDecodeError as e:
                raise Exception(f"Error decoding JSON from {result_path}: {e}")
            except Exception as e:
                raise Exception(f"Error reading {result_path}: {e}")
        

        if (job_data and 
            job_data.get("original_state") and 
            job_data["original_state"].get("agent_identifier")
            ):
            agent_id = job_data["original_state"]["agent_identifier"]
            agent = self.agents.get(agent_id)
            if not agent:
                self.logger.info(f"Agent {agent_id} not found for resuming job {job_id}.")
                return
            self.logger.info(f"Resuming job {job_id} for agent {agent_id}.")

            agent.pending_job_id = job_id

            def run_generation():
                generator = agent.wait_for_tool_completion(job_id)
                for _ in generator:
                    pass  # Discard output during forced resume
                    # Poller will pick up the updated state

                self.forced_resume_in_progress = False
                self.logger.info(f"Resumed generation for job {job_id} completed.")

            threading.Thread(target=run_generation, daemon=True).start()
        else:
            self.logger.info(f"No valid original state found in job {job_id}. Cannot resume.")
            self.forced_resume_in_progress = False

    def check_background_updates(self):
        """Polled by Gradio Timer."""
        full_history = self.get_agent_history(self.active_agent.__class__.__name__)
        
        if self.forced_resume_in_progress:
            return (
                full_history, 
                gr.update(interactive=False, placeholder="Resuming with previous job's results..."),
                gr.update(active=True) # Keep the timer running
            )
        else:
            return (
                full_history, 
                gr.update(interactive=True, placeholder="Type here..."), 
                gr.update(active=False) # Stop the timer
            )

    def export_all_histories(self):
        """
        Dumps the full message history of all agents to a JSON file for download.
        """
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = os.path.join("exports", f"session_history_{timestamp}.json")
        os.makedirs("exports", exist_ok=True)
        
        # Collect data
        export_data = {
            "timestamp": timestamp,
            "agents": {}
        }
        
        for name, agent in self.agents.items():
            export_data["agents"][name] = agent.messages
            
        # Write to file
        with open(filename, "w") as f:
            json.dump(export_data, f, indent=2)
            
        return filename
    
    def import_all_histories(self, filepath):
        """
        Loads message history from a JSON file and restores agent states.
        """
        if not filepath:
            return "No file provided."

        self.logger.info(f"Attempting to import session from {filepath}")

        try:
            with open(filepath, "r", encoding='utf-8') as f:
                data = json.load(f)
            
            # Validate format
            if "agents" not in data:
                return "Error: Invalid session file format (missing 'agents' key)."

            # Restore history for each agent found in the file
            loaded_count = 0
            for agent_name, messages in data["agents"].items():
                if agent_name in self.agents:
                    self.agents[agent_name].messages = messages
                    loaded_count += 1
                    self.logger.info(f"Restored {len(messages)} messages for {agent_name}.")
                else:
                    self.logger.warning(f"Agent '{agent_name}' found in file but not in current framework. Skipping.")
            
            return f"Successfully restored session (Timestamp: {data.get('timestamp', 'unknown')}). Loaded {loaded_count} agents."

        except json.JSONDecodeError:
            return "Error: Failed to decode JSON file."
        except Exception as e:
            self.logger.error(f"Import failed: {str(e)}")
            return f"Error importing history: {str(e)}"
    
    def get_gallery_images(self):
        """
        Scans current directory and toolout folders for generated plots.
        """
        # Search for standard plot formats in likely locations
        image_paths = []
        
        # 1. Check Root (where lacathode_roc.png is saved)
        image_paths.extend(glob.glob("*.png"))
        image_paths.extend(glob.glob("*.jpg"))
        
        # 2. Check Tool Output directories recursively
        image_paths.extend(glob.glob("toolout/**/*.png", recursive=True))
        image_paths.extend(glob.glob("toolout/**/*.jpg", recursive=True))
        
        # Sort by modification time (newest first)
        image_paths.sort(key=os.path.getmtime, reverse=True)
        return image_paths
    
    def chat_function(self, user_input, chat_history):
        message_id = uuid.uuid4().hex
        parsed_generator = self.active_agent.respond(user_input, message_id)

        for multiple_parsed in parsed_generator:
            full_history = self.active_agent.messages_to_gradio_history()
            # Transient bubbles
            current_bubbles = []
            for parsed in multiple_parsed:
                self.logger.debug(parsed)
                role = parsed.get("role", "assistant")
                if parsed["thinking"]:
                    current_bubbles.append(gr.ChatMessage(role=role, content=parsed["thinking"], metadata={"title": "Thinking"}))

                if parsed.get("tool_calls"):
                    for tool_call in parsed["tool_calls"]:
                        tool_call_name = "unknown_tool"

                        # Handle both object and dict formats just in case
                        if hasattr(tool_call, 'function'):
                            tool_call_name = tool_call.function.name
                        elif isinstance(tool_call, dict) and 'function' in tool_call:
                            tool_call_name = tool_call['function'].get('name', "unknown_tool")

                        current_bubbles.append(
                            gr.ChatMessage(
                                role=role, 
                                content=f"Invoking {tool_call_name}...", 
                                metadata={"title": "Tool Call"})
                            )
                        
                if parsed.get("content"):
                    current_bubbles.append(gr.ChatMessage(role=role, content=parsed["content"]))
            
            if current_bubbles:
                # Full history might already have the last message, so avoid duplication
                if (full_history and 
                    full_history[-1].content == current_bubbles[-1].content):
                    yield full_history
                else:
                    yield full_history + current_bubbles

    def get_agent_history(self, agent_name):
        agent = self.agents.get(agent_name)
        if agent:
            return agent.messages_to_gradio_history()
        return []

    def run_interactive(self, port=7860):
        with gr.Blocks(fill_height=True) as demo:
            with gr.Tabs():
                    with gr.Tab("Command Center", scale=1):
                        with gr.Row(scale=1):
                            with gr.Column(scale=1):
                                gr.Markdown("### üß† Orchestrator (Scientist)")
                                chatbot_active = gr.Chatbot(
                                    value=self.get_agent_history(self.active_agent.__class__.__name__), 
                                    type="messages", 
                                    label="Orchestrator",
                                    scale=1
                                )
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### üõ†Ô∏è Analytics (Technician)")
                                chatbot_side = gr.Chatbot(
                                    value=self.get_agent_history("AnalyticsAgent"), 
                                    type="messages",
                                    label="Analytics",
                                    scale=1
                                )

                        with gr.Row(scale=0):
                            #chatbot = gr.Chatbot(value=initial_gradio_history, type="messages", scale=1)
                            with gr.Column(scale=4):
                                msg = gr.Textbox(label="Your Input", placeholder="Type here...", autofocus=True)
                            
                            with gr.Column(scale=1):
                                submit_btn = gr.Button("Submit", variant="primary")

                                export_btn = gr.DownloadButton("üíæ Download Full Session History")

                                # Hook up the export button
                                export_btn.click(
                                    fn=self.export_all_histories,
                                    inputs=None,
                                    outputs=export_btn
                                )

                            def submit_wrapper(user_input):
                                gen = self.chat_function(user_input, [])
                                final_history = None
                                for history in gen:
                                    final_history = history
                                    yield (
                                        gr.update(value="", interactive=False), 
                                        history
                                    )

                                yield (
                                    gr.update(value="", interactive=True), 
                                    final_history
                                )

                            msg.submit(
                                fn=submit_wrapper, 
                                inputs=[msg], 
                                outputs=[msg, chatbot_active]
                            )

                            submit_btn.click(
                                fn=submit_wrapper, 
                                inputs=[msg], 
                                outputs=[msg, chatbot_active]
                            )

                            # Timer for background updates (Resume logic)
                            if self.forced_resume_in_progress:
                                active_agent_polling_timer = gr.Timer(value=0.5, active=True)
                                active_agent_polling_timer.tick(fn=self.check_background_updates, inputs=None, outputs=[chatbot_active, msg, active_agent_polling_timer])

                            side_agent_polling_timer = gr.Timer(value=1.0, active=True)
                            side_agent_polling_timer.tick(
                                fn=lambda: self.get_agent_history("AnalyticsAgent"),
                                inputs=None,
                                outputs=chatbot_side
                            )

                    with gr.Tab("Visualizations", scale=1):
                        gr.Markdown("### Generated Plots & Figures")
                        refresh_btn = gr.Button("üîÑ Refresh Gallery")
                        gallery = gr.Gallery(
                            label="Generated Images", 
                            show_label=False, 
                            elem_id="gallery", 
                            columns=[3], 
                            rows=[2], 
                            object_fit="contain", 
                            height="auto"
                        )
                        
                        # Load images on click
                        refresh_btn.click(
                            fn=self.get_gallery_images,
                            inputs=None,
                            outputs=gallery
                        )
                    
                    with gr.Tab("Tools", scale=1):
                        # Import Session
                        gr.Markdown("### üìÇ Import Previous Session")
                        def handle_import(file_obj):
                            # Run the import
                            status = self.import_all_histories(file_obj)
                            
                            # Return updates for both chatbots to show restored history immediately
                            return (
                                gr.update(placeholder=f"System: {status}"), # Feedback in the text box
                                self.get_agent_history(self.active_agent.__class__.__name__), # Refresh Active
                                self.get_agent_history("AnalyticsAgent") # Refresh Side
                            )

                        import_btn = gr.File(
                            label="üìÇ Restore Session", 
                            file_types=[".json"], 
                            file_count="single",
                            type="filepath" # Passes the file path string to the function
                        )
                                    
                        import_btn.upload(
                            fn=handle_import,
                            inputs=[import_btn],
                            outputs=[msg, chatbot_active, chatbot_side]
                        )
                    
                        gr.Markdown("### üìö External Knowledge Logs (Gemma)")
                        gr.Markdown("This log shows the specific questions asked to the external knowledge base and the responses received.")
                        
                        refresh_tools_btn = gr.Button("üîÑ Refresh Logs")
                        tools_log_display = gr.Markdown("No logs yet...")
                        
                        # Load initial state
                        demo.load(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )

                        # Manual Refresh
                        refresh_tools_btn.click(
                            fn=get_runtime_history_file,
                            inputs=None,
                            outputs=tools_log_display
                        )
        demo.launch(share=False, server_port=port)